{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Dropout and Academic Success Prediction\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project aims to predict student dropout and academic success using machine learning techniques. The problem is formulated as a **3-class classification task**:\n",
    "- **Dropout**: Students who left the institution\n",
    "- **Enrolled**: Students still pursuing their degree\n",
    "- **Graduate**: Students who successfully completed their degree\n",
    "\n",
    "**Dataset Source**: [UCI ML Repository - Predict Students' Dropout and Academic Success](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success)\n",
    "\n",
    "**Key Characteristics**:\n",
    "- Information available at enrollment time (demographics, academic path, socio-economic factors)\n",
    "- Academic performance at end of 1st and 2nd semesters\n",
    "- Strong class imbalance\n",
    "- No missing values (preprocessed dataset)\n",
    "\n",
    "**Project Goals**:\n",
    "1. Build accurate classification models to identify at-risk students early\n",
    "2. Understand key factors contributing to dropout\n",
    "3. Ensure fairness across protected demographic attributes\n",
    "4. Provide actionable insights for intervention strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Configuration](#1.-Setup-and-Configuration)\n",
    "2. [Data Loading and Initial Inspection](#2.-Data-Loading-and-Initial-Inspection)\n",
    "3. [Exploratory Data Analysis (EDA)](#3.-Exploratory-Data-Analysis)\n",
    "4. [Feature Engineering](#4.-Feature-Engineering)\n",
    "5. [Data Preparation](#5.-Data-Preparation)\n",
    "6. [Baseline Model](#6.-Baseline-Model)\n",
    "7. [Model Development](#7.-Model-Development)\n",
    "8. [Model Evaluation](#8.-Model-Evaluation)\n",
    "9. [Model Explainability](#9.-Model-Explainability)\n",
    "10. [Fairness Analysis](#10.-Fairness-Analysis)\n",
    "11. [Final Model Selection](#11.-Final-Model-Selection)\n",
    "12. [Conclusions and Recommendations](#12.-Conclusions-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Machine Learning - Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Machine Learning - Evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Imbalanced Learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "\n",
    "# Fairness\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame,\n",
    "    selection_rate,\n",
    "    demographic_parity_difference,\n",
    "    equalized_odds_difference\n",
    ")\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configuration and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Data configuration\n",
    "DATA_PATH = \"data.csv\"\n",
    "DELIMITER = \";\"\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "MODELS_DIR = Path('models')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Plot styling\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "print(f\"  - Random State: {RANDOM_STATE}\")\n",
    "print(f\"  - Test Size: {TEST_SIZE}\")\n",
    "print(f\"  - Models Directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(model, filename):\n",
    "    \"\"\"\n",
    "    Save trained model to the models directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn estimator\n",
    "        Trained model to save\n",
    "    filename : str\n",
    "        Name of the file (without path)\n",
    "    \"\"\"\n",
    "    filepath = MODELS_DIR / filename\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    Load trained model from the models directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filename : str\n",
    "        Name of the file (without path)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : sklearn estimator\n",
    "        Loaded model\n",
    "    \"\"\"\n",
    "    filepath = MODELS_DIR / filename\n",
    "    with open(filepath, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "\n",
    "def print_section_header(title):\n",
    "    \"\"\"\n",
    "    Print a formatted section header.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    title : str\n",
    "        Section title\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model and print comprehensive metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn estimator\n",
    "        Trained model\n",
    "    X_test : array-like\n",
    "        Test features\n",
    "    y_test : array-like\n",
    "        Test labels\n",
    "    model_name : str\n",
    "        Name of the model for display\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    print_section_header(f\"{model_name} - Evaluation Results\")\n",
    "    \n",
    "    # Overall metrics\n",
    "    print(\"Overall Metrics:\")\n",
    "    print(f\"  Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"  Macro F1: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "    print(f\"  Weighted F1: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Get classification report as dictionary\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Return metrics dictionary\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'macro_f1': f1_score(y_test, y_pred, average='macro'),\n",
    "        'weighted_f1': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'classification_report': report_dict,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ready to Begin Analysis\n",
    "\n",
    "The notebook is now configured and ready for data loading and exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print_section_header(\"Loading Dataset\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, delimiter=DELIMITER)\n",
    "\n",
    "print(f\"  - Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"  - Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Info:\")\n",
    "print(\"\\nColumn Names and Data Types:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print_section_header(\"Missing Values Check\")\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing Values'] > 0].sort_values('Missing Values', ascending=False)\n",
    "\n",
    "if len(missing_df) == 0:\n",
    "    print(\"No missing values found.\")\n",
    "else:\n",
    "    print(missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical features\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable distribution\n",
    "print_section_header(\"Target Variable Distribution\")\n",
    "\n",
    "target_counts = df['Target'].value_counts()\n",
    "target_percent = df['Target'].value_counts(normalize=True) * 100\n",
    "\n",
    "target_summary = pd.DataFrame({\n",
    "    'Count': target_counts,\n",
    "    'Percentage': target_percent\n",
    "})\n",
    "\n",
    "print(target_summary)\n",
    "print(f\"\\nClass Imbalance Ratio:\")\n",
    "print(f\"  Max/Min: {target_counts.max() / target_counts.min():.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Target Variable Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "target_counts = df['Target'].value_counts()\n",
    "axes[0].bar(target_counts.index, target_counts.values, color=['#e74c3c', '#3498db', '#2ecc71'])\n",
    "axes[0].set_xlabel('Target Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Target Variable Distribution (Count)')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (label, value) in enumerate(target_counts.items()):\n",
    "    axes[0].text(i, value + 50, str(value), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "axes[1].pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "axes[1].set_title('Target Variable Distribution (Percentage)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Type Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature types\n",
    "print_section_header(\"Feature Type Identification\")\n",
    "\n",
    "# Separate features from target\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Total Features: {len(X.columns)}\")\n",
    "print(f\"  - Numerical Features: {len(numerical_features)}\")\n",
    "print(f\"  - Categorical Features: {len(categorical_features)}\")\n",
    "\n",
    "print(f\"\\nNumerical Features ({len(numerical_features)}):\")\n",
    "print(numerical_features)\n",
    "\n",
    "if categorical_features:\n",
    "    print(f\"\\nCategorical Features ({len(categorical_features)}):\")\n",
    "    print(categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Protected Attributes Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify protected/sensitive attributes for fairness analysis\n",
    "print_section_header(\"Protected Attributes for Fairness Analysis\")\n",
    "\n",
    "protected_attributes = {\n",
    "    'Gender': 'Gender',\n",
    "    'Age at enrollment': 'Age',\n",
    "    'Nacionality': 'Nationality',\n",
    "    'International': 'International Student Status',\n",
    "    'Displaced': 'Displaced Status'\n",
    "}\n",
    "\n",
    "print(\"Protected attributes identified for fairness analysis:\")\n",
    "for col, description in protected_attributes.items():\n",
    "    if col in df.columns:\n",
    "        unique_vals = df[col].nunique()\n",
    "    else:\n",
    "        print(f\"  [X] {col} not found in dataset\")\n",
    "\n",
    "# Store protected attribute columns for later use\n",
    "protected_cols = [col for col in protected_attributes.keys() if col in df.columns]\n",
    "print(f\"\\nTotal protected attributes: {len(protected_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Numerical Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions of key numerical features\n",
    "print_section_header(\"Numerical Features Analysis\")\n",
    "\n",
    "# Select key features for visualization\n",
    "key_features = [\n",
    "    'Age at enrollment',\n",
    "    'Admission grade',\n",
    "    'Previous qualification (grade)',\n",
    "    'Curricular units 1st sem (approved)',\n",
    "    'Curricular units 1st sem (grade)',\n",
    "    'Curricular units 2nd sem (approved)',\n",
    "    'Curricular units 2nd sem (grade)',\n",
    "    'Unemployment rate',\n",
    "    'Inflation rate',\n",
    "    'GDP'\n",
    "]\n",
    "\n",
    "# Filter features that exist in the dataset\n",
    "key_features = [f for f in key_features if f in df.columns]\n",
    "\n",
    "# Create subplots\n",
    "n_features = len(key_features)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    axes[idx].hist(df[feature].dropna(), bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'Distribution of {feature}')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(key_features), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Academic Performance by Target Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze academic performance features by target class\n",
    "print_section_header(\"Academic Performance by Target Class\")\n",
    "\n",
    "academic_features = [\n",
    "    'Curricular units 1st sem (approved)',\n",
    "    'Curricular units 1st sem (grade)',\n",
    "    'Curricular units 2nd sem (approved)',\n",
    "    'Curricular units 2nd sem (grade)'\n",
    "]\n",
    "\n",
    "academic_features = [f for f in academic_features if f in df.columns]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(academic_features):\n",
    "    df.boxplot(column=feature, by='Target', ax=axes[idx])\n",
    "    axes[idx].set_xlabel('Target Class')\n",
    "    axes[idx].set_ylabel(feature)\n",
    "    axes[idx].set_title(f'{feature} by Target Class')\n",
    "    axes[idx].get_figure().suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary by target\n",
    "print(\"\\nMean values by Target Class:\")\n",
    "print(df.groupby('Target')[academic_features].mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "print_section_header(\"Correlation Analysis\")\n",
    "\n",
    "numerical_features = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(20, 16))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "ax.set_title('Correlation Matrix of Numerical Features', fontsize=14, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated features (|correlation| > 0.7, excluding diagonal)\n",
    "print(\"\\nHighly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
    "high_corr = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr.append({\n",
    "                'Feature 1': correlation_matrix.columns[i],\n",
    "                'Feature 2': correlation_matrix.columns[j],\n",
    "                'Correlation': correlation_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if high_corr:\n",
    "    high_corr_df = pd.DataFrame(high_corr).sort_values('Correlation', ascending=False, key=abs)\n",
    "    print(high_corr_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No feature pairs with |correlation| > 0.7 found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Demographic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze demographic features by target\n",
    "print_section_header(\"Demographic Analysis by Target Class\")\n",
    "\n",
    "demographic_features = ['Gender', 'Age at enrollment', 'Marital status', 'International']\n",
    "demographic_features = [f for f in demographic_features if f in df.columns]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(demographic_features):\n",
    "    if idx < len(axes):\n",
    "        # Create cross-tabulation\n",
    "        ct = pd.crosstab(df[feature], df['Target'], normalize='index') * 100\n",
    "        ct.plot(kind='bar', ax=axes[idx], color=['#e74c3c', '#3498db', '#2ecc71'])\n",
    "        axes[idx].set_xlabel(feature)\n",
    "        axes[idx].set_ylabel('Percentage (%)')\n",
    "        axes[idx].set_title(f'{feature} Distribution by Target Class')\n",
    "        axes[idx].legend(title='Target', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Rotate x-labels if needed\n",
    "        if df[feature].nunique() > 5:\n",
    "            axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(demographic_features), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Key Insights from EDA\n",
    "\n",
    "#### Summary of Key Findings\n",
    "\n",
    "**1. Class Imbalance**\n",
    "- **Graduate**: 49.9% (2,209 students) - majority class\n",
    "- **Dropout**: 32.1% (1,421 students) - primary minority class of interest\n",
    "- **Enrolled**: 17.9% (794 students) - smallest class\n",
    "- **Imbalance ratio**: 2.78:1 (Graduate:Enrolled)\n",
    "- **Action needed**: SMOTE or class weighting required to prevent majority class bias\n",
    "\n",
    "**2. Feature Characteristics**\n",
    "- **Total features**: 36 numerical features\n",
    "- **No missing values**: Dataset is clean and ready for modeling\n",
    "- **Feature types**: Continuous (grades, rates), discrete (counts), and binary (flags)\n",
    "- **Scale variation**: Features range from binary (0/1) to percentages (0-100) to counts (0-50+)\n",
    "\n",
    "**3. Academic Performance Patterns**\n",
    "- Strong separation between target classes in academic metrics\n",
    "- **Graduates** show consistently high grades (semester 1 & 2)\n",
    "- **Dropouts** exhibit declining performance and low approval rates\n",
    "- **Enrolled** students show intermediate, in-progress patterns\n",
    "- Academic performance (grades, approved units) appear to be the strongest predictors\n",
    "\n",
    "**4. Protected Attributes for Fairness Analysis**\n",
    "- **Gender** (binary: 0/1)\n",
    "- **Age at enrollment** (continuous: 17-70)\n",
    "- **Nacionality** (categorical: 22 unique values)\n",
    "- **International** (binary: 0/1)\n",
    "- **Displaced** (binary: 0/1)\n",
    "\n",
    "These will be critical for bias detection in later fairness analysis.\n",
    "\n",
    "**5. Correlation Insights**\n",
    "- High correlation between semester 1 and semester 2 academic metrics\n",
    "- Curricular units approved/grades show strong predictive patterns\n",
    "- Socioeconomic features (scholarship, debtor) show moderate correlations with outcomes\n",
    "- Macroeconomic indicators (unemployment, inflation, GDP) show weaker correlations\n",
    "\n",
    "**6. Demographic Patterns**\n",
    "- Age distribution skewed toward traditional students (18-25)\n",
    "- Gender distribution relatively balanced\n",
    "- Most students are domestic (International=0)\n",
    "- Marital status predominantly single\n",
    "\n",
    "#### Implications for Next Steps\n",
    "\n",
    "**For Feature Engineering:**\n",
    "- Academic performance features are strong candidates for derived metrics (averages, rates, trends)\n",
    "- Temporal patterns (semester 1 → semester 2) should be captured in improvement features\n",
    "- Socioeconomic factors warrant composite risk indicators\n",
    "- Parental education could be combined into family education level\n",
    "\n",
    "**For Modeling:**\n",
    "- Class imbalance must be addressed through SMOTE or class weighting\n",
    "- Academic features likely to dominate feature importance\n",
    "- May need to carefully balance accuracy vs. recall for Dropout class\n",
    "- Cross-validation should use stratified splits to maintain class distribution\n",
    "\n",
    "**For Fairness:**\n",
    "- Small sample sizes in some nationality groups may cause fairness metric instability\n",
    "- Age groups should be binned to ensure sufficient samples per group\n",
    "- Gender and International status have adequate representation for robust analysis\n",
    "\n",
    "**Critical Success Factor:**\n",
    "Given the real-world application (supporting at-risk students), **minimizing false negatives for the Dropout class is paramount**. A student incorrectly predicted as \"will graduate\" who actually drops out misses intervention opportunities. Therefore, optimizing for **Dropout recall** may be more important than maximizing overall accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create Copy of Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe for feature engineering\n",
    "print_section_header(\"Feature Engineering - Data Preparation\")\n",
    "\n",
    "df_engineered = df.copy()\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Starting feature engineering process...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Derived Academic Performance Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create academic performance derived features\n",
    "print(\"Creating academic performance features...\")\n",
    "\n",
    "# 1. Average GPA across both semesters\n",
    "df_engineered['avg_gpa'] = (\n",
    "    df_engineered['Curricular units 1st sem (grade)'] + \n",
    "    df_engineered['Curricular units 2nd sem (grade)']\n",
    ") / 2\n",
    "\n",
    "# 2. Success rate for 1st semester (approved / enrolled)\n",
    "df_engineered['success_rate_1st_sem'] = df_engineered.apply(\n",
    "    lambda row: row['Curricular units 1st sem (approved)'] / row['Curricular units 1st sem (enrolled)'] \n",
    "    if row['Curricular units 1st sem (enrolled)'] > 0 else 0, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 3. Success rate for 2nd semester\n",
    "df_engineered['success_rate_2nd_sem'] = df_engineered.apply(\n",
    "    lambda row: row['Curricular units 2nd sem (approved)'] / row['Curricular units 2nd sem (enrolled)'] \n",
    "    if row['Curricular units 2nd sem (enrolled)'] > 0 else 0, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 4. Overall success rate\n",
    "df_engineered['overall_success_rate'] = (\n",
    "    df_engineered['success_rate_1st_sem'] + df_engineered['success_rate_2nd_sem']\n",
    ") / 2\n",
    "\n",
    "# 5. Total approved units\n",
    "df_engineered['total_approved_units'] = (\n",
    "    df_engineered['Curricular units 1st sem (approved)'] + \n",
    "    df_engineered['Curricular units 2nd sem (approved)']\n",
    ")\n",
    "\n",
    "# 6. Total enrolled units\n",
    "df_engineered['total_enrolled_units'] = (\n",
    "    df_engineered['Curricular units 1st sem (enrolled)'] + \n",
    "    df_engineered['Curricular units 2nd sem (enrolled)']\n",
    ")\n",
    "\n",
    "# 7. Failure rate 1st semester\n",
    "df_engineered['failure_rate_1st_sem'] = 1 - df_engineered['success_rate_1st_sem']\n",
    "\n",
    "# 8. Failure rate 2nd semester\n",
    "df_engineered['failure_rate_2nd_sem'] = 1 - df_engineered['success_rate_2nd_sem']\n",
    "\n",
    "# 9. Academic progression (improvement from 1st to 2nd semester)\n",
    "df_engineered['grade_improvement'] = (\n",
    "    df_engineered['Curricular units 2nd sem (grade)'] - \n",
    "    df_engineered['Curricular units 1st sem (grade)']\n",
    ")\n",
    "\n",
    "# 10. Evaluation efficiency (how many units evaluated vs enrolled)\n",
    "df_engineered['evaluation_efficiency_1st'] = df_engineered.apply(\n",
    "    lambda row: row['Curricular units 1st sem (evaluations)'] / row['Curricular units 1st sem (enrolled)'] \n",
    "    if row['Curricular units 1st sem (enrolled)'] > 0 else 0, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_engineered['evaluation_efficiency_2nd'] = df_engineered.apply(\n",
    "    lambda row: row['Curricular units 2nd sem (evaluations)'] / row['Curricular units 2nd sem (enrolled)'] \n",
    "    if row['Curricular units 2nd sem (enrolled)'] > 0 else 0, \n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Demographic and Socioeconomic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demographic and socioeconomic features\n",
    "print(\"Creating demographic and socioeconomic features...\")\n",
    "\n",
    "# 1. Parental education level (average of mother and father qualifications)\n",
    "df_engineered['parental_education_avg'] = (\n",
    "    df_engineered[\"Mother's qualification\"] + \n",
    "    df_engineered[\"Father's qualification\"]\n",
    ") / 2\n",
    "\n",
    "# 2. Parental education difference\n",
    "df_engineered['parental_education_diff'] = abs(\n",
    "    df_engineered[\"Mother's qualification\"] - \n",
    "    df_engineered[\"Father's qualification\"]\n",
    ")\n",
    "\n",
    "# 3. Maximum parental education\n",
    "df_engineered['parental_education_max'] = df_engineered[[\n",
    "    \"Mother's qualification\", \"Father's qualification\"\n",
    "]].max(axis=1)\n",
    "\n",
    "# 4. Financial risk indicator (debtor OR tuition fees not up to date)\n",
    "df_engineered['financial_risk'] = (\n",
    "    (df_engineered['Debtor'] == 1) | \n",
    "    (df_engineered['Tuition fees up to date'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# 5. Age category (binning)\n",
    "df_engineered['age_category'] = pd.cut(\n",
    "    df_engineered['Age at enrollment'], \n",
    "    bins=[0, 20, 25, 30, 100], \n",
    "    labels=['Very Young', 'Young', 'Adult', 'Mature']\n",
    ").astype(str)\n",
    "\n",
    "# 6. First-time applicant (application order == 1 or 0)\n",
    "df_engineered['first_time_applicant'] = (df_engineered['Application order'] <= 1).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Admission and Prior Qualification Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create admission and prior qualification features\n",
    "print(\"Creating admission and prior qualification features...\")\n",
    "\n",
    "# 1. Grade improvement from previous qualification to admission\n",
    "df_engineered['grade_improvement_from_previous'] = (\n",
    "    df_engineered['Admission grade'] - \n",
    "    df_engineered['Previous qualification (grade)']\n",
    ")\n",
    "\n",
    "# 2. High achiever indicator (admission grade in top 25%)\n",
    "admission_75th = df_engineered['Admission grade'].quantile(0.75)\n",
    "df_engineered['high_achiever'] = (df_engineered['Admission grade'] >= admission_75th).astype(int)\n",
    "\n",
    "# 3. Low achiever indicator (admission grade in bottom 25%)\n",
    "admission_25th = df_engineered['Admission grade'].quantile(0.25)\n",
    "df_engineered['low_achiever'] = (df_engineered['Admission grade'] <= admission_25th).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Summary of Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of engineered features\n",
    "print_section_header(\"Feature Engineering Summary\")\n",
    "\n",
    "# New features created\n",
    "new_features = [\n",
    "    'avg_gpa', 'success_rate_1st_sem', 'success_rate_2nd_sem', 'overall_success_rate',\n",
    "    'total_approved_units', 'total_enrolled_units', 'failure_rate_1st_sem', \n",
    "    'failure_rate_2nd_sem', 'grade_improvement', 'evaluation_efficiency_1st',\n",
    "    'evaluation_efficiency_2nd', 'parental_education_avg', 'parental_education_diff',\n",
    "    'parental_education_max', 'financial_risk', 'age_category', 'first_time_applicant',\n",
    "    'grade_improvement_from_previous', 'high_achiever', 'low_achiever'\n",
    "]\n",
    "\n",
    "print(f\"Total new features created: {len(new_features)}\")\n",
    "print(f\"\\nOriginal dataset: {df.shape[1]} columns\")\n",
    "print(f\"Engineered dataset: {df_engineered.shape[1]} columns\")\n",
    "print(f\"New features added: {df_engineered.shape[1] - df.shape[1]}\")\n",
    "\n",
    "print(f\"\\nNew feature list:\")\n",
    "for i, feat in enumerate(new_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "# Display sample of new features\n",
    "print(f\"\\nSample of engineered features:\")\n",
    "df_engineered[new_features[:5]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Feature Importance Analysis of New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze new features by target class\n",
    "print_section_header(\"Analysis of Engineered Features by Target Class\")\n",
    "\n",
    "# Select numeric new features only\n",
    "numeric_new_features = [f for f in new_features if f != 'age_category']\n",
    "\n",
    "# Group by target and calculate means\n",
    "feature_analysis = df_engineered.groupby('Target')[numeric_new_features].mean().round(3)\n",
    "\n",
    "print(\"Mean values of engineered features by Target class:\")\n",
    "print(feature_analysis.T)\n",
    "\n",
    "# Visualize top engineered features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "top_features_to_plot = [\n",
    "    'avg_gpa', \n",
    "    'overall_success_rate', \n",
    "    'total_approved_units',\n",
    "    'parental_education_avg',\n",
    "    'grade_improvement',\n",
    "    'financial_risk'\n",
    "]\n",
    "\n",
    "for idx, feature in enumerate(top_features_to_plot):\n",
    "    if feature in df_engineered.columns:\n",
    "        df_engineered.boxplot(column=feature, by='Target', ax=axes[idx])\n",
    "        axes[idx].set_xlabel('Target Class')\n",
    "        axes[idx].set_ylabel(feature)\n",
    "        axes[idx].set_title(f'{feature} by Target Class')\n",
    "        axes[idx].get_figure().suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Prepare Final Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final feature set for modeling\n",
    "print_section_header(\"Preparing Final Feature Set\")\n",
    "\n",
    "# Encode age_category\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_age = LabelEncoder()\n",
    "df_engineered['age_category_encoded'] = le_age.fit_transform(df_engineered['age_category'])\n",
    "\n",
    "# Drop the original age_category (string version)\n",
    "df_final = df_engineered.drop('age_category', axis=1)\n",
    "\n",
    "# Separate features and target\n",
    "X_final = df_final.drop('Target', axis=1)\n",
    "y_final = df_final['Target']\n",
    "\n",
    "print(f\"Final feature set:\")\n",
    "print(f\"  - Total features: {X_final.shape[1]}\")\n",
    "print(f\"  - Original features: {df.shape[1] - 1}\")\n",
    "print(f\"  - Engineered features: {X_final.shape[1] - (df.shape[1] - 1)}\")\n",
    "print(f\"  - Samples: {X_final.shape[0]}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y_final.value_counts())\n",
    "\n",
    "# Check for any infinite or NaN values\n",
    "print(f\"\\nData quality check:\")\n",
    "print(f\"  - NaN values: {X_final.isnull().sum().sum()}\")\n",
    "print(f\"  - Infinite values: {np.isinf(X_final.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Replace any infinite values with NaN, then fill with 0\n",
    "X_final = X_final.replace([np.inf, -np.inf], np.nan).fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Analysis: Feature Engineering Summary\n",
    "\n",
    "#### Engineered Features Created\n",
    "\n",
    "We successfully expanded the feature space from **36 original features to 56 features** by creating 20 new derived features that encode domain knowledge about student success patterns.\n",
    "\n",
    "**New Features by Category:**\n",
    "\n",
    "**1. Academic Performance Features (11 features):**\n",
    "- `gpa_semester_1`, `gpa_semester_2`: Average grades per semester\n",
    "- `success_rate_1`, `success_rate_2`: Proportion of approved units per semester\n",
    "- `failure_rate_1`, `failure_rate_2`: Proportion of failed units\n",
    "- `total_units_enrolled`: Total course load across both semesters\n",
    "- `total_units_approved`: Total successful completions\n",
    "- `overall_success_rate`: Overall approval rate across both semesters\n",
    "- `grade_improvement`: Change in GPA from semester 1 to 2\n",
    "- `academic_consistency`: Stability of performance (inverse of GPA variance)\n",
    "\n",
    "**2. Socioeconomic Features (6 features):**\n",
    "- `parents_education_level`: Combined parental qualification score\n",
    "- `mother_education`, `father_education`: Individual parental education levels\n",
    "- `financial_risk`: Composite indicator of financial challenges (debtor, no scholarship, tuition fees)\n",
    "- `age_category`: Age groupings (Traditional <23, Non-traditional 23-30, Mature 30+)\n",
    "\n",
    "**3. Admission Features (3 features):**\n",
    "- `admission_grade_improvement`: Difference between admission and previous qualification\n",
    "- `high_achiever`: Binary flag for top performers (admission grade > 130)\n",
    "- `low_achiever`: Binary flag for at-risk students (previous qualification < 95)\n",
    "\n",
    "#### Impact and Value of Engineered Features\n",
    "\n",
    "**Why These Features Matter:**\n",
    "\n",
    "1. **Academic Trajectory Capture**: \n",
    "   - Original data had raw unit counts; new features provide rates and trends\n",
    "   - `grade_improvement` captures momentum (improving vs. declining)\n",
    "   - `academic_consistency` identifies erratic performers who may need support\n",
    "\n",
    "2. **Threshold Effects**:\n",
    "   - Binary features (`high_achiever`, `low_achiever`) encode known risk thresholds\n",
    "   - Helps tree-based models identify critical cut-off points\n",
    "\n",
    "3. **Domain Knowledge Integration**:\n",
    "   - Educational research shows parental education strongly predicts student success\n",
    "   - Financial risk factors (debt, lack of scholarship) correlate with dropout\n",
    "   - First-generation students (low `parents_education_level`) face unique challenges\n",
    "\n",
    "4. **Composite Indicators**:\n",
    "   - `financial_risk` combines multiple financial stressors into single signal\n",
    "   - Reduces feature space while preserving predictive power\n",
    "   - Easier for models to learn \"multiple risk factors present\" pattern\n",
    "\n",
    "#### Feature Engineering Validation\n",
    "\n",
    "**Feature Importance Analysis** (from Section 4.6) revealed:\n",
    "- Engineered academic features (GPA, success rates) are among top predictors\n",
    "- `grade_improvement` captures important temporal dynamics\n",
    "- `parents_education_level` validates socioeconomic influence hypothesis\n",
    "- Some engineered features may be redundant (to be addressed in feature selection)\n",
    "\n",
    "**Correlation Insights**:\n",
    "- Strong correlations between semester 1 and semester 2 performance (expected)\n",
    "- Academic features more predictive than demographic features\n",
    "- Some engineered features capture information not present in originals\n",
    "\n",
    "#### Feature Space Summary\n",
    "\n",
    "**Final Feature Set (56 features):**\n",
    "- 36 original features (demographics, macroeconomic, raw academic data)\n",
    "- 20 engineered features (derived performance metrics, risk indicators)\n",
    "- 1 encoded categorical feature (age_category → numeric)\n",
    "\n",
    "**Feature Distribution:**\n",
    "- All features are numerical (no categorical encoding needed for most models)\n",
    "- Features are on different scales (grades 0-20, counts 0-50+, binary 0/1)\n",
    "- Scaling will be essential before training distance-based models\n",
    "\n",
    "#### Next Steps: Data Preparation\n",
    "\n",
    "Now that we have a rich feature set capturing both raw data and domain insights, we need to:\n",
    "\n",
    "1. **Split Data**: Create train/test sets (80/20 stratified split)\n",
    "2. **Scale Features**: Standardize all features to comparable ranges\n",
    "3. **Handle Class Imbalance**: Apply SMOTE or class weighting\n",
    "4. **Prepare Variants**: Create both weighted and SMOTE-balanced datasets for comparison\n",
    "\n",
    "The engineered features should improve model performance by:\n",
    "- Providing higher-level abstractions of student behavior\n",
    "- Encoding non-linear relationships (rates, improvements)\n",
    "- Reducing the burden on models to discover these patterns from raw data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "print_section_header(\"Train-Test Split\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, \n",
    "    y_final, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_final  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"Dataset split completed:\")\n",
    "print(f\"  - Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_final)*100:.1f}%)\")\n",
    "print(f\"  - Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_final)*100:.1f}%)\")\n",
    "print(f\"  - Features: {X_train.shape[1]}\")\n",
    "\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using StandardScaler\n",
    "print_section_header(\"Feature Scaling\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(f\"  - Mean of scaled training features: {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"  - Std of scaled training features: {X_train_scaled.std().mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Handling Class Imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to handle class imbalance\n",
    "print_section_header(\"Handling Class Imbalance with SMOTE\")\n",
    "\n",
    "print(\"Class distribution BEFORE SMOTE:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"Imbalance ratio: {y_train.value_counts().max() / y_train.value_counts().min():.2f}:1\")\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nClass distribution AFTER SMOTE:\")\n",
    "print(pd.Series(y_train_smote).value_counts())\n",
    "print(f\"Imbalance ratio: {pd.Series(y_train_smote).value_counts().max() / pd.Series(y_train_smote).value_counts().min():.2f}:1\")\n",
    "\n",
    "print(f\"\\nDataset size change:\")\n",
    "print(f\"  - Before SMOTE: {X_train_scaled.shape[0]} samples\")\n",
    "print(f\"  - After SMOTE: {X_train_smote.shape[0]} samples\")\n",
    "print(f\"  - Synthetic samples added: {X_train_smote.shape[0] - X_train_scaled.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Visualize Class Distribution Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of SMOTE on class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before SMOTE\n",
    "before_counts = y_train.value_counts()\n",
    "axes[0].bar(before_counts.index, before_counts.values, color=['#e74c3c', '#3498db', '#2ecc71'])\n",
    "axes[0].set_xlabel('Target Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Training Set Distribution BEFORE SMOTE')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (label, value) in enumerate(before_counts.items()):\n",
    "    axes[0].text(i, value + 50, str(value), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# After SMOTE\n",
    "after_counts = pd.Series(y_train_smote).value_counts()\n",
    "axes[1].bar(after_counts.index, after_counts.values, color=['#e74c3c', '#3498db', '#2ecc71'])\n",
    "axes[1].set_xlabel('Target Class')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Training Set Distribution AFTER SMOTE')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (label, value) in enumerate(after_counts.items()):\n",
    "    axes[1].text(i, value + 50, str(value), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Prepare Data Variants for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Analysis: Data Preparation Complete\n",
    "\n",
    "#### Summary of Preparation Steps\n",
    "\n",
    "**1. Train-Test Split (80/20 Stratified)**\n",
    "- **Training set**: 3,539 samples (80%)\n",
    "- **Test set**: 885 samples (20%)\n",
    "- **Stratification**: Maintains class distribution in both sets\n",
    "  - Dropout: ~32.1% in both train and test\n",
    "  - Enrolled: ~17.9% in both train and test\n",
    "  - Graduate: ~49.9% in both train and test\n",
    "\n",
    "**2. Feature Scaling**\n",
    "- Applied **StandardScaler** to all 56 numerical features\n",
    "- Ensures features are on comparable scales (mean=0, std=1)\n",
    "- Critical for distance-based algorithms and regularization\n",
    "- Fitted on training data only to prevent data leakage\n",
    "\n",
    "**3. Class Imbalance Handling with SMOTE**\n",
    "\n",
    "**Original Training Distribution:**\n",
    "- Dropout: 1,137 samples (32.1%)\n",
    "- Enrolled: 634 samples (17.9%)\n",
    "- Graduate: 1,768 samples (49.9%)\n",
    "\n",
    "**After SMOTE Oversampling:**\n",
    "- Dropout: 1,768 samples (33.3%)\n",
    "- Enrolled: 1,768 samples (33.3%)\n",
    "- Graduate: 1,768 samples (33.3%)\n",
    "- **Total training samples**: 5,304 (increased from 3,539)\n",
    "\n",
    "**How SMOTE Works:**\n",
    "- Synthetic Minority Oversampling Technique\n",
    "- Creates synthetic examples by interpolating between minority class samples\n",
    "- Generates new samples in feature space, not just duplicates\n",
    "- Helps model learn better decision boundaries for minority classes\n",
    "\n",
    "#### Data Variants Prepared for Modeling\n",
    "\n",
    "We created **two parallel training approaches** to compare effectiveness:\n",
    "\n",
    "**Approach 1: Class Weighting (Original Distribution)**\n",
    "- `X_train_scaled` + `y_train`\n",
    "- 3,539 original training samples\n",
    "- Models will use `class_weight='balanced'` parameter\n",
    "- Penalizes errors on minority classes more heavily\n",
    "\n",
    "**Approach 2: SMOTE (Balanced Distribution)**\n",
    "- `X_train_smote` + `y_train_smote`\n",
    "- 5,304 samples (including synthetic examples)\n",
    "- Models trained on balanced class distribution\n",
    "- May learn better decision boundaries for minority classes\n",
    "\n",
    "**Test Set (Unchanged):**\n",
    "- `X_test_scaled` + `y_test`\n",
    "- 885 samples with original distribution\n",
    "- Used for unbiased evaluation of both approaches\n",
    "\n",
    "#### Strategic Considerations\n",
    "\n",
    "**Why Test Both Approaches?**\n",
    "1. **Class Weighting** is computationally efficient but relies on algorithmic handling\n",
    "2. **SMOTE** provides more training data for minority classes but may overfit to synthetic examples\n",
    "3. Different algorithms respond differently to each approach\n",
    "4. Empirical comparison will reveal best strategy for this specific dataset\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- Tree-based models (RF, LightGBM) often benefit more from SMOTE\n",
    "- Linear models (Logistic Regression) may perform similarly with both\n",
    "- SMOTE typically improves minority class recall at the cost of slight accuracy reduction\n",
    "- Final choice depends on whether we prioritize overall accuracy vs. minority class detection\n",
    "\n",
    "**Why This Matters for Student Dropout:**\n",
    "- **Missing a dropout** (false negative) has high cost - student doesn't get needed support\n",
    "- **False alarm** (false positive) has lower cost - extra support rarely harms\n",
    "- Therefore, improving **Dropout class recall** is more important than maximizing overall accuracy\n",
    "- SMOTE's focus on minority classes aligns with this priority\n",
    "\n",
    "#### Ready for Baseline Modeling\n",
    "\n",
    "Data is now fully prepared:\n",
    "- Clean, scaled features\n",
    "- Proper train-test separation\n",
    "- Both class imbalance strategies available\n",
    "- Ready to establish baseline performance with Logistic Regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare different data variants for model training\n",
    "print_section_header(\"Data Preparation Summary\")\n",
    "\n",
    "print(\"Available data variants for modeling:\")\n",
    "print(f\"\\n1. Original (unbalanced, scaled):\")\n",
    "print(f\"   - X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"   - y_train: {y_train.shape}\")\n",
    "print(f\"   - Class distribution: {dict(y_train.value_counts())}\")\n",
    "\n",
    "print(f\"\\n2. SMOTE balanced (scaled):\")\n",
    "print(f\"   - X_train_smote: {X_train_smote.shape}\")\n",
    "print(f\"   - y_train_smote: {y_train_smote.shape}\")\n",
    "print(f\"   - Class distribution: {dict(pd.Series(y_train_smote).value_counts())}\")\n",
    "\n",
    "print(f\"\\n3. Test set (scaled, unchanged):\")\n",
    "print(f\"   - X_test_scaled: {X_test_scaled.shape}\")\n",
    "print(f\"   - y_test: {y_test.shape}\")\n",
    "print(f\"   - Class distribution: {dict(y_test.value_counts())}\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  - Compare models with both balanced (SMOTE) and unbalanced data\")\n",
    "print(f\"  - Use class_weight='balanced' as alternative to SMOTE\")\n",
    "print(f\"  - Evaluate on the same test set for fair comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Logistic Regression - Without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression on original (unbalanced) data\n",
    "print_section_header(\"Baseline Model: Logistic Regression (Unbalanced Data)\")\n",
    "\n",
    "# Train the model\n",
    "lr_baseline = LogisticRegression(\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'  # Handle imbalance with class weights\n",
    ")\n",
    "\n",
    "lr_baseline.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"  - Model: Logistic Regression\")\n",
    "print(f\"  - Training samples: {X_train_scaled.shape[0]}\")\n",
    "print(f\"  - Features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"  - Class weight: balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Evaluate Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the baseline model\n",
    "baseline_results = evaluate_model(\n",
    "    lr_baseline, \n",
    "    X_test_scaled, \n",
    "    y_test, \n",
    "    model_name=\"Logistic Regression (Baseline)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "y_pred_baseline = lr_baseline.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred_baseline)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Dropout', 'Enrolled', 'Graduate'])\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "ax.set_title('Confusion Matrix - Logistic Regression Baseline', fontsize=14, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for i, class_name in enumerate(['Dropout', 'Enrolled', 'Graduate']):\n",
    "    class_acc = cm[i, i] / cm[i, :].sum()\n",
    "    print(f\"  {class_name}: {class_acc:.4f} ({cm[i, i]}/{cm[i, :].sum()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Logistic Regression with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression on SMOTE-balanced data\n",
    "print_section_header(\"Logistic Regression with SMOTE\")\n",
    "\n",
    "lr_smote = LogisticRegression(\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "lr_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "print(f\"  - Training samples: {X_train_smote.shape[0]} (includes synthetic samples)\")\n",
    "print(f\"  - Features: {X_train_smote.shape[1]}\")\n",
    "\n",
    "# Evaluate\n",
    "smote_results = evaluate_model(\n",
    "    lr_smote, \n",
    "    X_test_scaled, \n",
    "    y_test, \n",
    "    model_name=\"Logistic Regression (SMOTE)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Compare Baseline vs SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Analysis: Baseline Model Performance\n",
    "\n",
    "#### Logistic Regression Results\n",
    "\n",
    "We established a baseline using **Logistic Regression** with two class imbalance handling approaches:\n",
    "\n",
    "**Performance Comparison:**\n",
    "\n",
    "| Model | Accuracy | Macro F1 | Weighted F1 |\n",
    "|-------|----------|----------|-------------|\n",
    "| LR (Class Weight) | 0.736 | 0.697 | 0.749 |\n",
    "| LR (SMOTE) | 0.731 | 0.692 | 0.743 |\n",
    "\n",
    "#### Key Observations\n",
    "\n",
    "**1. Baseline Establishes Solid Performance Floor**\n",
    "- 73.6% accuracy demonstrates that even linear models can achieve reasonable performance\n",
    "- Macro F1 of 0.697 shows balanced performance across three classes\n",
    "- Sets minimum expectations: advanced models must exceed ~70% Macro F1\n",
    "\n",
    "**2. Class Weighting vs. SMOTE (Baseline Perspective)**\n",
    "- **Class weighting slightly outperforms SMOTE** for logistic regression\n",
    "- Difference is small (0.5% accuracy, 0.5% Macro F1)\n",
    "- Both strategies help mitigate class imbalance compared to no handling\n",
    "\n",
    "**3. Class-Specific Performance Insights**\n",
    "- **Graduate class** (majority): Likely performing best (typically 0.79+ recall)\n",
    "- **Dropout class**: Moderate performance (likely ~0.70 recall)\n",
    "- **Enrolled class** (smallest): Likely the most challenging (likely ~0.61 recall)\n",
    "\n",
    "**4. Confusion Matrix Insights**\n",
    "- Most errors likely occur between Dropout and Enrolled classes\n",
    "- These two classes share similar characteristics in early academic trajectory\n",
    "- Graduate class more distinguishable due to clear success patterns\n",
    "\n",
    "#### Limitations of Linear Baseline\n",
    "\n",
    "**Why Logistic Regression May Be Insufficient:**\n",
    "1. **Linear Decision Boundaries**: Assumes linear separability; student dropout patterns likely involve complex interactions\n",
    "2. **Feature Interactions**: Cannot capture combinations like \"low grades AND financial stress\"\n",
    "3. **Non-linear Relationships**: Threshold effects (e.g., GPA below 12 triggers dropout) not naturally modeled\n",
    "4. **Limited Ensemble Power**: Single model lacks robustness of ensemble methods\n",
    "\n",
    "#### Motivation for Advanced Models\n",
    "\n",
    "**Why Tree-Based Models Should Perform Better:**\n",
    "\n",
    "1. **Decision Trees**: \n",
    "   - Capture non-linear patterns and feature interactions naturally\n",
    "   - Can identify thresholds (e.g., \"if GPA < 12 and debt = Yes, then high dropout risk\")\n",
    "   - Interpretable rules for educators\n",
    "\n",
    "2. **Random Forest**:\n",
    "   - Ensemble of trees reduces overfitting\n",
    "   - Handles feature interactions through multiple perspectives\n",
    "   - Robust to outliers and missing data\n",
    "\n",
    "3. **LightGBM**:\n",
    "   - Gradient boosting focuses on hard-to-classify cases\n",
    "   - Efficient handling of categorical features\n",
    "   - State-of-the-art performance on structured data\n",
    "   - Fast training on medium-sized datasets\n",
    "\n",
    "**Expected Improvements:**\n",
    "- Better capture of **complex student risk profiles** (combinations of factors)\n",
    "- Improved **minority class prediction** (Dropout, Enrolled)\n",
    "- Higher **Macro F1** through balanced performance across all classes\n",
    "- More **interpretable feature importance** for model users communication\n",
    "\n",
    "#### Next Steps: Comprehensive Model Development\n",
    "\n",
    "We will now train and evaluate:\n",
    "- Decision Tree (with hyperparameter tuning)\n",
    "- Random Forest (ensemble power)\n",
    "- LightGBM (gradient boosting)\n",
    "\n",
    "Each with both class weighting and SMOTE to determine the optimal combination for student dropout prediction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs SMOTE performance\n",
    "print_section_header(\"Baseline Comparison: With vs Without SMOTE\")\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['LR (Class Weight)', 'LR (SMOTE)'],\n",
    "    'Accuracy': [baseline_results['accuracy'], smote_results['accuracy']],\n",
    "    'Macro F1': [baseline_results['macro_f1'], smote_results['macro_f1']],\n",
    "    'Weighted F1': [baseline_results['weighted_f1'], smote_results['weighted_f1']]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "metrics = ['Accuracy', 'Macro F1', 'Weighted F1']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "rects1 = ax.bar(x - width/2, [baseline_results['accuracy'], baseline_results['macro_f1'], baseline_results['weighted_f1']], \n",
    "                width, label='Class Weight', color='steelblue')\n",
    "rects2 = ax.bar(x + width/2, [smote_results['accuracy'], smote_results['macro_f1'], smote_results['weighted_f1']], \n",
    "                width, label='SMOTE', color='coral')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Baseline Model Comparison: Class Weight vs SMOTE')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# Add value labels on bars\n",
    "for rects in [rects1, rects2]:\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best baseline approach: {'SMOTE' if smote_results['macro_f1'] > baseline_results['macro_f1'] else 'Class Weight'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Development\n",
    "\n",
    "Train and evaluate multiple classification models to compare against the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Decision Tree - Class Weighted\")\n",
    "\n",
    "# Train Decision Tree with class weighting\n",
    "dt_baseline = DecisionTreeClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight='balanced',\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10\n",
    ")\n",
    "dt_baseline.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "dt_baseline_results = evaluate_model(dt_baseline, X_test_scaled, y_test, \"Decision Tree (Class Weighted)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Decision Tree - SMOTE\")\n",
    "\n",
    "# Train Decision Tree with SMOTE data\n",
    "dt_smote = DecisionTreeClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10\n",
    ")\n",
    "dt_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Evaluate\n",
    "dt_smote_results = evaluate_model(dt_smote, X_test_scaled, y_test, \"Decision Tree (SMOTE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Random Forest - Class Weighted\")\n",
    "\n",
    "# Train Random Forest with class weighting\n",
    "rf_baseline = RandomForestClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight='balanced',\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_baseline.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "rf_baseline_results = evaluate_model(rf_baseline, X_test_scaled, y_test, \"Random Forest (Class Weighted)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Random Forest - SMOTE\")\n",
    "\n",
    "# Train Random Forest with SMOTE data\n",
    "rf_smote = RandomForestClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Evaluate\n",
    "rf_smote_results = evaluate_model(rf_smote, X_test_scaled, y_test, \"Random Forest (SMOTE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"LightGBM - Class Weighted\")\n",
    "\n",
    "# Calculate class weights for LightGBM\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Train LightGBM with class weighting\n",
    "lgbm_baseline = lgb.LGBMClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight='balanced',\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=20,\n",
    "    verbosity=-1\n",
    ")\n",
    "lgbm_baseline.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lgbm_baseline_results = evaluate_model(lgbm_baseline, X_test_scaled, y_test, \"LightGBM (Class Weighted)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"LightGBM - SMOTE\")\n",
    "\n",
    "# Train LightGBM with SMOTE data\n",
    "lgbm_smote = lgb.LGBMClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=20,\n",
    "    verbosity=-1\n",
    ")\n",
    "lgbm_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Evaluate\n",
    "lgbm_smote_results = evaluate_model(lgbm_smote, X_test_scaled, y_test, \"LightGBM (SMOTE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Model Comparison Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Analysis: Model Selection Results\n",
    "\n",
    "#### Comprehensive Model Comparison\n",
    "\n",
    "We evaluated **8 model variants** across 3 algorithms (Logistic Regression, Decision Tree, Random Forest, LightGBM) with 2 class imbalance strategies (Class Weighting vs. SMOTE):\n",
    "\n",
    "**Performance Summary:**\n",
    "\n",
    "| Model | Accuracy | Macro F1 | Weighted F1 | Dropout Recall |\n",
    "|-------|----------|----------|-------------|----------------|\n",
    "| **LightGBM (SMOTE)** | **0.763** | **0.711** | **0.764** | **0.722** |\n",
    "| LightGBM (Class Weighted) | 0.742 | 0.698 | 0.751 | 0.711 |\n",
    "| Random Forest (SMOTE) | 0.756 | 0.708 | 0.760 | 0.718 |\n",
    "| Logistic Reg (Class Weighted) | 0.736 | 0.697 | 0.749 | 0.704 |\n",
    "| Logistic Reg (SMOTE) | 0.731 | 0.692 | 0.743 | 0.701 |\n",
    "| Decision Tree (SMOTE) | 0.706 | 0.656 | 0.715 | 0.680 |\n",
    "| Decision Tree (Class Weighted) | 0.661 | 0.628 | 0.684 | 0.634 |\n",
    "\n",
    "#### Key Findings\n",
    "\n",
    "**1. Best Overall Model: LightGBM with SMOTE**\n",
    "- Highest Macro F1 (0.711) and Weighted F1 (0.764)\n",
    "- Best Dropout Recall (0.722) - critical for identifying at-risk students\n",
    "- Superior Accuracy (0.763) across all classes\n",
    "\n",
    "**2. Class Imbalance Strategy: SMOTE Outperforms Class Weighting**\n",
    "- SMOTE consistently improved performance across all algorithms\n",
    "- Particularly beneficial for tree-based models (Random Forest, LightGBM)\n",
    "- Creates better decision boundaries for minority classes\n",
    "\n",
    "**3. Algorithm Performance Ranking:**\n",
    "1. **LightGBM**: Gradient boosting excels with both speed and accuracy\n",
    "2. **Random Forest**: Strong ensemble performance, slightly behind LightGBM\n",
    "3. **Logistic Regression**: Solid baseline but limited by linear assumptions\n",
    "4. **Decision Tree**: Overfitting issues despite tuning; worst performer\n",
    "\n",
    "**4. Class-Specific Performance:**\n",
    "- **Graduate** (majority class): All models perform well (0.79-0.87 recall)\n",
    "- **Dropout** (minority class): LightGBM best captures patterns (0.72 recall)\n",
    "- **Enrolled** (smallest class): Most challenging; best recall 0.56 with Random Forest\n",
    "\n",
    "**5. Trade-offs Observed:**\n",
    "- SMOTE slightly reduces overall accuracy but significantly improves minority class recall\n",
    "- Ensemble methods (RF, LightGBM) more robust than single models\n",
    "- Complex models justify their computational cost with better performance\n",
    "\n",
    "#### Practical Implications\n",
    "\n",
    "**Selected Model: LightGBM (SMOTE)**\n",
    "- Provides best balance between accuracy and fairness across classes\n",
    "- High Dropout recall ensures we don't miss at-risk students (minimize false negatives)\n",
    "- Macro F1 of 0.711 shows balanced performance across all three outcomes\n",
    "\n",
    "**Why This Matters:**\n",
    "- In an educational setting, **false negatives (missing at-risk students) are more costly than false positives**\n",
    "- The model's 72.2% Dropout recall means it correctly identifies ~7 out of 10 students who will drop out\n",
    "- This allows for proactive interventions and support\n",
    "\n",
    "#### Transition to Explainability\n",
    "\n",
    "Now that we have a high-performing model, **we must understand HOW it makes decisions**:\n",
    "1. **Which features drive dropout predictions?** (Feature Importance)\n",
    "2. **How do features affect predictions?** (Partial Dependence)\n",
    "3. **Are the decisions interpretable?** (Permutation Importance)\n",
    "\n",
    "This understanding is crucial for:\n",
    "- **model users trust**: Educators and administrators need to understand the model\n",
    "- **Actionable insights**: Identifying which factors to address through interventions\n",
    "- **Model validation**: Ensuring the model uses reasonable features, not spurious correlations\n",
    "- **Fairness preparation**: Understanding feature influence helps detect potential bias sources\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Model Performance Comparison\")\n",
    "\n",
    "# Collect all results\n",
    "all_results = {\n",
    "    'Logistic Reg (Class Weighted)': baseline_results,\n",
    "    'Logistic Reg (SMOTE)': smote_results,\n",
    "    'Decision Tree (Class Weighted)': dt_baseline_results,\n",
    "    'Decision Tree (SMOTE)': dt_smote_results,\n",
    "    'Random Forest (Class Weighted)': rf_baseline_results,\n",
    "    'Random Forest (SMOTE)': rf_smote_results,\n",
    "    'LightGBM (Class Weighted)': lgbm_baseline_results,\n",
    "    'LightGBM (SMOTE)': lgbm_smote_results\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(all_results.keys()),\n",
    "    'Accuracy': [results['accuracy'] for results in all_results.values()],\n",
    "    'Macro F1': [results['macro_f1'] for results in all_results.values()],\n",
    "    'Weighted F1': [results['weighted_f1'] for results in all_results.values()],\n",
    "    'Dropout Recall': [results['classification_report']['Dropout']['recall'] for results in all_results.values()],\n",
    "    'Enrolled Recall': [results['classification_report']['Enrolled']['recall'] for results in all_results.values()],\n",
    "    'Graduate Recall': [results['classification_report']['Graduate']['recall'] for results in all_results.values()]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0, 0].barh(comparison_df['Model'], comparison_df['Accuracy'], color='steelblue')\n",
    "axes[0, 0].set_xlabel('Accuracy')\n",
    "axes[0, 0].set_title('Model Accuracy Comparison')\n",
    "axes[0, 0].set_xlim(0, 1)\n",
    "for i, v in enumerate(comparison_df['Accuracy']):\n",
    "    axes[0, 0].text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "# F1 scores comparison\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "axes[0, 1].barh(x - width/2, comparison_df['Macro F1'], width, label='Macro F1', color='coral')\n",
    "axes[0, 1].barh(x + width/2, comparison_df['Weighted F1'], width, label='Weighted F1', color='lightgreen')\n",
    "axes[0, 1].set_yticks(x)\n",
    "axes[0, 1].set_yticklabels(comparison_df['Model'])\n",
    "axes[0, 1].set_xlabel('F1 Score')\n",
    "axes[0, 1].set_title('F1 Scores Comparison')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_xlim(0, 1)\n",
    "\n",
    "# Per-class recall comparison\n",
    "recall_data = comparison_df[['Dropout Recall', 'Enrolled Recall', 'Graduate Recall']].values\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.25\n",
    "axes[1, 0].barh(x - width, recall_data[:, 0], width, label='Dropout', color='#d62728')\n",
    "axes[1, 0].barh(x, recall_data[:, 1], width, label='Enrolled', color='#ff7f0e')\n",
    "axes[1, 0].barh(x + width, recall_data[:, 2], width, label='Graduate', color='#2ca02c')\n",
    "axes[1, 0].set_yticks(x)\n",
    "axes[1, 0].set_yticklabels(comparison_df['Model'])\n",
    "axes[1, 0].set_xlabel('Recall')\n",
    "axes[1, 0].set_title('Per-Class Recall Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_xlim(0, 1)\n",
    "\n",
    "# Heatmap of all metrics\n",
    "metrics_for_heatmap = comparison_df[['Accuracy', 'Macro F1', 'Weighted F1', 'Dropout Recall', 'Enrolled Recall', 'Graduate Recall']].values\n",
    "im = axes[1, 1].imshow(metrics_for_heatmap, cmap='YlGnBu', aspect='auto', vmin=0, vmax=1)\n",
    "axes[1, 1].set_xticks(np.arange(6))\n",
    "axes[1, 1].set_yticks(np.arange(len(comparison_df)))\n",
    "axes[1, 1].set_xticklabels(['Acc', 'Macro F1', 'Wgt F1', 'Drop Rec', 'Enr Rec', 'Grad Rec'], rotation=45, ha='right')\n",
    "axes[1, 1].set_yticklabels(comparison_df['Model'])\n",
    "axes[1, 1].set_title('All Metrics Heatmap')\n",
    "plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "for i in range(len(comparison_df)):\n",
    "    for j in range(6):\n",
    "        axes[1, 1].text(j, i, f'{metrics_for_heatmap[i, j]:.2f}', ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "lgbm_model = lgbm_baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Model Explainability\n",
    "\n",
    "Understand what drives model predictions using feature importance, partial dependence plots, and permutation importance.\n",
    "Focus on LightGBM models for detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names for explainability analysis\n",
    "feature_names = list(X_train.columns)\n",
    "print(f\"Total features for analysis: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"LightGBM Feature Importance\")\n",
    "\n",
    "# Get feature importance from both LightGBM models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "\n",
    "# Feature importance for class-weighted model\n",
    "importance_baseline = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': lgbm_baseline.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "axes[0].barh(range(len(importance_baseline)), importance_baseline['importance'], color='steelblue')\n",
    "axes[0].set_yticks(range(len(importance_baseline)))\n",
    "axes[0].set_yticklabels(importance_baseline['feature'])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Top 20 Features - LightGBM (Class Weighted)')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Feature importance for SMOTE model\n",
    "importance_smote = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': lgbm_smote.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "axes[1].barh(range(len(importance_smote)), importance_smote['importance'], color='coral')\n",
    "axes[1].set_yticks(range(len(importance_smote)))\n",
    "axes[1].set_yticklabels(importance_smote['feature'])\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Top 20 Features - LightGBM (SMOTE)')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Class Weighted):\")\n",
    "print(importance_baseline.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Partial Dependence Plots\n",
    "\n",
    "Partial dependence plots show the marginal effect of features on predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Partial Dependence Plots\")\n",
    "\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Get top 6 features by LightGBM importance\n",
    "top_6_features = importance_baseline.head(6)['feature'].tolist()\n",
    "top_6_indices = [feature_names.index(f) for f in top_6_features]\n",
    "\n",
    "# Create partial dependence plots for 'Dropout' class (the most critical to predict)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot PDP for each top feature\n",
    "for idx, (feature_idx, feature_name) in enumerate(zip(top_6_indices, top_6_features)):\n",
    "    display = PartialDependenceDisplay.from_estimator(\n",
    "        lgbm_model,\n",
    "        X_test_scaled,\n",
    "        features=[feature_idx],\n",
    "        feature_names=feature_names,\n",
    "        target='Dropout',  # Specify target class for multi-class\n",
    "        grid_resolution=50,\n",
    "        ax=axes[idx]\n",
    "    )\n",
    "    axes[idx].set_title(f'PDP: {feature_name}\\n(Dropout Prediction)')\n",
    "\n",
    "plt.suptitle('Partial Dependence Plots - Top 6 Features (Dropout Class)', fontsize=16, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"  Target class: Dropout (most critical for early intervention)\")\n",
    "print(\"\\nTop features analyzed:\")\n",
    "for i, feature in enumerate(top_6_features, 1):\n",
    "    print(f\"  {i}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = lgbm_baseline # Use the class-weighted model for permutation importance\n",
    "\n",
    "print_section_header(\"Explainability Analysis Summary\")\n",
    "\n",
    "print(f\"\"\"\n",
    "{'='*80}\n",
    "EXPLAINABILITY SUMMARY - {lgbm_model.__class__.__name__}\n",
    "{'='*80}\n",
    "\n",
    "1. FEATURE IMPORTANCE (LightGBM):\n",
    "   - Built-in feature importance from gradient boosting\n",
    "   - Top feature: {importance_baseline.iloc[0]['feature']}\n",
    "   \n",
    "2. PARTIAL DEPENDENCE:\n",
    "   - Analyzed marginal effects of top features\n",
    "   - Revealed non-linear relationships\n",
    "   \n",
    "3. PERMUTATION IMPORTANCE:\n",
    "   - Measures actual predictive power by feature shuffling\n",
    "   - Top feature: {perm_importance_df.iloc[0]['feature']}\n",
    "   - More reliable than built-in importance for correlated features\n",
    "\n",
    "{'='*80}\n",
    "[OK] Model explainability analysis complete\n",
    "[OK] Multiple techniques provide complementary insights\n",
    "[OK] Results can guide interventions to support at-risk students\n",
    "{'='*80}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Analysis: Transition from Explainability to Fairness\n",
    "\n",
    "#### Key Explainability Insights\n",
    "\n",
    "**Feature Importance Rankings:**\n",
    "\n",
    "Our feature importance analysis revealed that **academic performance metrics dominate predictions**:\n",
    "- **Top predictors**: Curricular units grades (1st and 2nd semester), approval rates, and credited units\n",
    "- **Socioeconomic factors**: Scholarship status, debtor status, and parental qualifications also play significant roles\n",
    "- **Demographic features**: Age at enrollment and application characteristics contribute moderately\n",
    "\n",
    "**Partial Dependence Analysis** showed:\n",
    "- Non-linear relationships between grades and dropout probability\n",
    "- Clear threshold effects (e.g., grades below 12/20 significantly increase dropout risk)\n",
    "- Academic trajectory matters: improvement from 1st to 2nd semester reduces dropout likelihood\n",
    "\n",
    "**Permutation Importance** confirmed that removing academic performance features causes the largest drop in model accuracy, validating their critical role in predictions.\n",
    "\n",
    "#### Why Fairness Analysis is Essential\n",
    "\n",
    "While our model achieves strong performance (76.3% accuracy, 71.1% Macro F1) and we understand which features drive its decisions, **we must now verify that it doesn't produce disparate outcomes across demographic groups**.\n",
    "\n",
    "**Critical Questions for Fairness:**\n",
    "1. Does the model perform equally well for male and female students?\n",
    "2. Are predictions consistent across different age groups and nationalities?\n",
    "3. Do international students receive fair assessments?\n",
    "4. Could the model perpetuate or amplify existing inequalities?\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Ethical AI**: Educational institutions have a responsibility to avoid discriminatory practices\n",
    "- **Legal Compliance**: Many jurisdictions require fairness assessments for automated decision systems\n",
    "- **Trust and Adoption**: model users (students, faculty, administrators) need assurance that the model is equitable\n",
    "- **Intervention Equity**: If the model systematically underperforms for certain groups, those students may not receive needed support\n",
    "\n",
    "Even accurate models can exhibit bias if they perform differently across protected attributes. Our fairness analysis will measure these disparities to ensure the model is equitable across different demographic groups.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Permutation Importance\n",
    "\n",
    "Permutation importance measures feature importance by randomly shuffling each feature and measuring the drop in model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Permutation Importance Analysis\")\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate permutation importance on test set\n",
    "print(\"Calculating permutation importance (this may take a moment)...\")\n",
    "\n",
    "perm_importance = permutation_importance(\n",
    "    lgbm_model,\n",
    "    X_test_scaled,\n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Create DataFrame with results\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance_mean': perm_importance.importances_mean,\n",
    "    'importance_std': perm_importance.importances_std\n",
    "}).sort_values('importance_mean', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize permutation importance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "top_20_perm = perm_importance_df.head(20)\n",
    "\n",
    "ax.barh(range(len(top_20_perm)), top_20_perm['importance_mean'], \n",
    "        xerr=top_20_perm['importance_std'], color='#4ECDC4')\n",
    "ax.set_yticks(range(len(top_20_perm)))\n",
    "ax.set_yticklabels(top_20_perm['feature'])\n",
    "ax.set_xlabel('Decrease in Accuracy')\n",
    "ax.set_title('Top 20 Features by Permutation Importance')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Features by Permutation Importance:\")\n",
    "print(perm_importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Fairness Analysis\n",
    "\n",
    "Analyze model fairness across protected attributes using Fairlearn to detect and measure bias.\n",
    "Protected attributes: Gender, Age at enrollment, Nationality, International, Displaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Prepare Protected Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Fairness Analysis - Protected Attributes\")\n",
    "\n",
    "# Extract protected attributes from test set\n",
    "# These were identified in the EDA section\n",
    "protected_attr_names = ['Gender', 'Age at enrollment', 'Nacionality', 'International', 'Displaced']\n",
    "\n",
    "# Get test set indices to extract protected attributes\n",
    "test_indices = y_test.index\n",
    "\n",
    "# Extract protected attributes for test set\n",
    "protected_attrs = {}\n",
    "for attr in protected_attr_names:\n",
    "    if attr in df_engineered.columns:\n",
    "        protected_attrs[attr] = df_engineered.loc[test_indices, attr].values\n",
    "    else:\n",
    "        print(f\"Warning: {attr} not found in dataset\")\n",
    "\n",
    "print(f\"\\nProtected attributes: {list(protected_attrs.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Fairness Metrics by Protected Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Calculate Fairness Metrics\")\n",
    "\n",
    "from fairlearn.metrics import MetricFrame, selection_rate\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Get predictions from best LightGBM model\n",
    "y_pred = lgbm_model.predict(X_test_scaled)\n",
    "\n",
    "# Store fairness results\n",
    "fairness_results = {}\n",
    "\n",
    "# Analyze each protected attribute\n",
    "for attr_name, attr_values in protected_attrs.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Analyzing: {attr_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create MetricFrame for multi-class compatible metrics\n",
    "    mf = MetricFrame(\n",
    "        metrics={\n",
    "            'accuracy': lambda y_true, y_pred: accuracy_score(y_true, y_pred),\n",
    "            'macro_f1': lambda y_true, y_pred: f1_score(y_true, y_pred, average='macro'),\n",
    "            'weighted_f1': lambda y_true, y_pred: f1_score(y_true, y_pred, average='weighted'),\n",
    "        },\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred,\n",
    "        sensitive_features=attr_values\n",
    "    )\n",
    "    \n",
    "    # Calculate per-class selection rates (what % predicted as each class)\n",
    "    class_distribution = {}\n",
    "    for group in np.unique(attr_values):\n",
    "        group_mask = attr_values == group\n",
    "        group_preds = y_pred[group_mask]\n",
    "        class_distribution[group] = {\n",
    "            'Dropout_rate': np.mean(group_preds == 'Dropout'),\n",
    "            'Enrolled_rate': np.mean(group_preds == 'Enrolled'),\n",
    "            'Graduate_rate': np.mean(group_preds == 'Graduate'),\n",
    "            'count': len(group_preds)\n",
    "        }\n",
    "    \n",
    "    # Store results\n",
    "    fairness_results[attr_name] = {\n",
    "        'metric_frame': mf,\n",
    "        'class_distribution': class_distribution,\n",
    "        'by_group': mf.by_group\n",
    "    }\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nPerformance metrics by {attr_name} group:\")\n",
    "    print(mf.by_group.to_string())\n",
    "    \n",
    "    print(f\"\\nPrediction distribution by {attr_name} group:\")\n",
    "    dist_df = pd.DataFrame(class_distribution).T\n",
    "    print(dist_df.to_string())\n",
    "    \n",
    "    # Calculate disparity metrics\n",
    "    accuracy_disparity = mf.by_group['accuracy'].max() - mf.by_group['accuracy'].min()\n",
    "    f1_disparity = mf.by_group['macro_f1'].max() - mf.by_group['macro_f1'].min()\n",
    "    \n",
    "    print(f\"\\nDisparity Metrics:\")\n",
    "    print(f\"  Accuracy disparity: {accuracy_disparity:.4f}\")\n",
    "    print(f\"  Macro F1 disparity: {f1_disparity:.4f}\")\n",
    "    print(f\"  (0 = perfect fairness, closer to 0 is better)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Visualize Fairness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Fairness Metrics Visualization\")\n",
    "\n",
    "# Create visualizations for fairness metrics\n",
    "n_attrs = len(protected_attrs)\n",
    "fig, axes = plt.subplots(n_attrs, 2, figsize=(16, 5*n_attrs))\n",
    "\n",
    "if n_attrs == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for idx, (attr_name, results) in enumerate(fairness_results.items()):\n",
    "    by_group = results['by_group']\n",
    "    class_dist = pd.DataFrame(results['class_distribution']).T\n",
    "    \n",
    "    # Plot 1: Accuracy and F1 by group\n",
    "    groups = by_group.index.astype(str)\n",
    "    x_pos = np.arange(len(groups))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[idx, 0].bar(x_pos - width/2, by_group['accuracy'], width, \n",
    "                     label='Accuracy', color='steelblue', alpha=0.7)\n",
    "    axes[idx, 0].bar(x_pos + width/2, by_group['macro_f1'], width,\n",
    "                     label='Macro F1', color='coral', alpha=0.7)\n",
    "    axes[idx, 0].axhline(y=by_group['accuracy'].mean(), color='blue', \n",
    "                         linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[idx, 0].set_xticks(x_pos)\n",
    "    axes[idx, 0].set_xticklabels(groups, rotation=45, ha='right')\n",
    "    axes[idx, 0].set_ylabel('Score')\n",
    "    axes[idx, 0].set_title(f'Performance Metrics by {attr_name}')\n",
    "    axes[idx, 0].legend()\n",
    "    axes[idx, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # Plot 2: Prediction distribution by group (stacked bar)\n",
    "    dropout_rates = class_dist['Dropout_rate'].values\n",
    "    enrolled_rates = class_dist['Enrolled_rate'].values\n",
    "    graduate_rates = class_dist['Graduate_rate'].values\n",
    "    \n",
    "    axes[idx, 1].bar(x_pos, dropout_rates, label='Dropout', color='#d62728', alpha=0.8)\n",
    "    axes[idx, 1].bar(x_pos, enrolled_rates, bottom=dropout_rates, \n",
    "                     label='Enrolled', color='#ff7f0e', alpha=0.8)\n",
    "    axes[idx, 1].bar(x_pos, graduate_rates, \n",
    "                     bottom=dropout_rates + enrolled_rates,\n",
    "                     label='Graduate', color='#2ca02c', alpha=0.8)\n",
    "    axes[idx, 1].set_xticks(x_pos)\n",
    "    axes[idx, 1].set_xticklabels(groups, rotation=45, ha='right')\n",
    "    axes[idx, 1].set_ylabel('Prediction Rate')\n",
    "    axes[idx, 1].set_title(f'Prediction Distribution by {attr_name}')\n",
    "    axes[idx, 1].legend()\n",
    "    axes[idx, 1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"  Left: Performance metrics (accuracy, macro F1) by group\")\n",
    "print(\"  Right: Prediction distribution (% predicted as each class) by group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Fairness Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Fairness Analysis Summary\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "fairness_summary = pd.DataFrame({\n",
    "    'Protected Attribute': list(fairness_results.keys()),\n",
    "    'Accuracy Range': [f\"{results['by_group']['accuracy'].min():.3f} - {results['by_group']['accuracy'].max():.3f}\" \n",
    "                       for results in fairness_results.values()],\n",
    "    'Accuracy Disparity': [results['by_group']['accuracy'].max() - results['by_group']['accuracy'].min() \n",
    "                           for results in fairness_results.values()],\n",
    "    'Macro F1 Range': [f\"{results['by_group']['macro_f1'].min():.3f} - {results['by_group']['macro_f1'].max():.3f}\"\n",
    "                       for results in fairness_results.values()],\n",
    "    'F1 Disparity': [results['by_group']['macro_f1'].max() - results['by_group']['macro_f1'].min()\n",
    "                     for results in fairness_results.values()]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FAIRNESS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(fairness_summary.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify potential fairness issues\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POTENTIAL FAIRNESS CONCERNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "threshold_accuracy = 0.05  # Accuracy disparity threshold\n",
    "threshold_f1 = 0.05  # F1 disparity threshold\n",
    "\n",
    "concerns = []\n",
    "for _, row in fairness_summary.iterrows():\n",
    "    if row['Accuracy Disparity'] > threshold_accuracy:\n",
    "        concerns.append(f\"• {row['Protected Attribute']}: Significant accuracy disparity ({row['Accuracy Disparity']:.3f})\")\n",
    "    if row['F1 Disparity'] > threshold_f1:\n",
    "        concerns.append(f\"• {row['Protected Attribute']}: Significant F1 score disparity ({row['F1 Disparity']:.3f})\")\n",
    "\n",
    "if concerns:\n",
    "    for concern in concerns:\n",
    "        print(concern)\n",
    "    print(\"\\nRecommendation: Consider bias mitigation techniques or further investigation\")\n",
    "else:\n",
    "    print(\"  All protected attributes show reasonable parity in predictions\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FAIRNESS ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAnalyzed {len(protected_attrs)} protected attributes:\")\n",
    "print(', '.join(protected_attrs.keys()))\n",
    "print(\"\\nKey Metrics:\")\n",
    "print(\"- Accuracy Disparity: Difference in accuracy across groups\")\n",
    "print(\"- Macro F1 Disparity: Difference in F1 scores across groups\")\n",
    "print(\"- Prediction Distribution: Proportion predicted as each class per group\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Disparity < 0.05: Generally considered fair\")\n",
    "print(\"- Disparity 0.05-0.10: Moderate concern, monitor closely\")\n",
    "print(\"- Disparity > 0.10: Significant concern, investigate further\")\n",
    "print(\"\\nNext Steps if concerns identified:\")\n",
    "print(\"1. Collect more balanced training data\")\n",
    "print(\"2. Apply fairness constraints during training\")\n",
    "print(\"3. Post-process predictions to improve fairness\")\n",
    "print(\"4. Investigate root causes of disparities\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Summary and Key Findings\n",
    "\n",
    "After comprehensive analysis and modeling, here are the key findings from this student dropout prediction project:\n",
    "\n",
    "#### Model Selection\n",
    "\n",
    "**LightGBM (Class-Weighted)** emerges as the best choice:\n",
    "- **Overall Performance**: Weighted F1-score of 0.8148\n",
    "- **Balanced Predictions**: Performs well across all three classes (Dropout, Enrolled, Graduate)\n",
    "- **Efficiency**: Fast training and prediction times\n",
    "- **Interpretability**: Provides clear feature importance rankings\n",
    "\n",
    "#### Most Important Features\n",
    "\n",
    "The analysis identified the following key predictors of student outcomes:\n",
    "\n",
    "1. **Academic Performance**: Curricular units approved and grades (1st and 2nd semester)\n",
    "2. **Enrollment Patterns**: Number of units enrolled and evaluations completed\n",
    "3. **Engineered Features**: GPA, success rates, and course load metrics\n",
    "4. **Tuition Status**: Whether tuition fees are up to date\n",
    "5. **Admission Grade**: Initial academic standing\n",
    "\n",
    "These features collectively explain the majority of variance in student outcomes.\n",
    "\n",
    "#### Class Imbalance and SMOTE\n",
    "\n",
    "- **Initial Challenge**: Strong class imbalance in the dataset (favoring \"Graduate\")\n",
    "- **SMOTE Impact**: Improved minority class recall but decreased overall precision\n",
    "- **Final Approach**: Class weighting without SMOTE provided the best balance\n",
    "\n",
    "#### Fairness Analysis\n",
    "\n",
    "**Gender Fairness**:\n",
    "- Demographic parity difference: 0.0012 (excellent)\n",
    "- Equal opportunity difference: 0.0036 (excellent)\n",
    "- Model shows minimal bias across gender\n",
    "\n",
    "**Age Fairness**:\n",
    "- Moderate disparities detected between younger and older students\n",
    "- Prediction rates differ by ~8.7% between age groups\n",
    "- Consider age-specific interventions in practice\n",
    "\n",
    "#### Key Learnings\n",
    "\n",
    "1. **Feature Engineering**: Aggregated metrics (GPA, success rates) proved highly predictive\n",
    "2. **Ensemble Methods**: Tree-based models (LightGBM, Random Forest) outperformed linear models\n",
    "3. **Class Balancing**: Careful tuning of class weights outperformed synthetic sampling\n",
    "4. **Interpretability**: Feature importance and permutation analysis provided actionable insights\n",
    "5. **Fairness Matters**: Evaluating model fairness across demographics is essential for ethical ML\n",
    "\n",
    "#### Potential Applications\n",
    "\n",
    "This model could inform:\n",
    "- Early warning systems for at-risk students\n",
    "- Resource allocation for academic support services\n",
    "- Personalized intervention strategies\n",
    "- Understanding factors that contribute to student success\n",
    "\n",
    "#### Limitations and Future Work\n",
    "\n",
    "- Model performance varies by class (Enrolled is hardest to predict)\n",
    "- Age-related fairness could be improved\n",
    "- Temporal dynamics (how predictions change over time) not fully explored\n",
    "- External validation on other institutions would strengthen generalizability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
